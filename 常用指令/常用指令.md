### SSH指令

ssh jiantong.zhao@hpclabs.mbzu.ae
ssh jiantong.zhao@rhpclabs.mbzu.ae

ssh jiantong@10.127.94.15
Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.15.0-79-generic x86_64)

ssh metaverse@10.127.94.15
mbzuai2023!



Mymachine

ssh jiantong@10.127.94.11

Oldmachine

 ssh jiantong@10.127.94.15

### Ubuntu启动故障

https://devicetests.com/fix-stuck-clean-message-boot-ubuntu

solution 4

### cuda

查看cuda版本

nvcc -V

export CUDA_HOME=/usr/local/cuda

export CUDA_HOME=/usr/local/cuda-11.6

在某终端切换cuda版本
source ~/local/switch-cuda.sh 12.1

check the symbolic link

code /usr/local/cuda/version.json

硬切换：
进入 ~/.bashrc 修改最后几行

code ~/.bashrc

完事以后 

source ~/.bashrc

echo $CUDA_HOME

To uninstall the CUDA Toolkit, run 

cuda-uninstaller in /usr/local/cuda-11.8/bin

临时切换cuda版本

```shell
export PATH=/usr/local/cuda-12.2/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH
```

修改软链接：

```shell
sudo unlink /usr/local/cuda

# or

sudo rm /usr/local/cuda

# Create a new symbolic link
sudo ln -s /usr/local/cuda-11.8 /usr/local/cuda

# Verify
ls -l /usr/local/cuda
# or more specifically
ls -l /usr/local | grep cuda

export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

source ~/.bashrc
```





### 进程管理

查看当前的python进程

ps -ef | grep python

查看某进程对应的用户

ps -f -p 进程PID



### conda虚拟环境

创建虚拟环境

conda create -n object-discovery python=3.9

```shell
conda create -n diffdope_test python=3.9
```

查看现有虚拟环境

conda info -e

删除虚拟环境

conda remove -n diffdope_test --all

虚拟环境改名

conda create --name nerf_studio --clone nerfstudio

conda create --name diffdope_test --clone diffdope

conda create --name interpose --clone diffdope_test

安装1.xx系列最新版pytorch

```sh
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
```

查看python interpreter 路径

which python3

/home/jiantong/local/miniconda3/envs/gaussian_splatting_debug/bin/python3

查看pip包安装路径:
pip show packagename

### 查看sudo权限
groups jiantong





### 网页端口映射

ssh -CNg -L 7007:127.0.0.1:7007 jiantong@10.127.94.15



### 查找目录

find /tmp -name CLEVR_v1.0 -type d

根据关键词过滤文件

ls /usr/lib/x86_64-linux-gnu | grep -i libegl

### screen相关指令

screen -S test -X quit



### 与本地传输文件

scp jiantong@10.127.94.15:~/nerfstudio/renders/cup_move/2023-08-31_201844.mp4 C:\Users\zjt00\Desktop\
scp -r jiantong@10.127.94.15:~/Colmap/tissue-bag-gaussian D:\COLMAP-3.8-windows-cuda\data\

scp -r C:\Users\zjt00\.ssh\id_rsa.pub jiantong@10.127.94.11:/home/jiantong/local/
scp C:\Users\zjt00\.ssh\id_rsa.pub jiantong@10.127.94.15:/home/jiantong/
scp -r C:\Users\zjt00\Downloads\OriginalCode.zip jiantong@10.127.94.11:/home/jiantong/project/python/
scp -r C:\Users\zjt00\Downloads\annotations jiantong@10.127.94.11:/home/jiantong/project/python/ELITE/datasets/Open_Images/annotations

scp -r C:\Users\zjt00\Desktop\Videos_Masdar jiantong@172.18.10.17:/home/jiantong/project/python/laptop_doc/

scp -r C:\Users\zjt00\Desktop\git学习笔记.md jiantong@10.127.94.11:/home/jiantong/project/python/laptop_doc/

scp -r jiantong@10.127.94.11:/home/jiantong/ELITE_Compare/ C:\Users\zjt00\Desktop\

scp -r jiantong@10.127.94.11:/home/jiantong/project/python/ELITE/outputs/ C:\Users\zjt00\Desktop\



scp -r mbzuai@10.67.31.18:/home/mbzuai/Jiantong/object-discovery-pytorch/checkpoints/clevr6_mask/2024-04-16_05-19-56 /home/jiantong/project/python/SAPI/JiantongZhao/object-discovery-pytorch/checkpoints/clevr6_mask

scp -r mbzuai@10.67.31.18:/home/mbzuai/Jiantong /home/jiantong/dataset/COCO2017.zip 

mv /home/jiantong/project/python/ELITE/Datadownloads/open-images-v6/test/data /home/jiantong/project/python/ELITE/datasets/Open_Images/images/

### ffmpeg convert video to image sequence

ffmpeg -i /home/jiantong/ffmpeg/Videos_Masdar/Incubator.MOV -qscale:v 1 -qmin 1 -vf fps=3 /home/jiantong/ffmpeg/Videos_Masdar/Incubator/%4d.jpg

### 从Google云盘下载文件到Ubuntu

pip install gdown

gdown https://drive.google.com/uc?id=1eDjh-_bxKKnEuz5h-HXS7EDJn59clx6V





### 服务器内文件操作

linux服务器内复制
cp -r ~/project/python/XMem/Segments/video3/ ~/nerfstudio/data_raw/

批量移动目录
find . -type d -exec mv evaluation masks_XMem models outdoor_ds.ckpt XMem-s012.pth /home/jiantong/project/python/ \;

删除软件

sudo apt-get --purge remove colmap

下载文件

wget https://cmake.org/download/cmake-3.27.4-linux-x86_64.sh

统计文件数

ls -1 /home/jiantong/project/python/XMem/Outputs/video3 | wc -l

显示文件大小

du -h filename

显示当前目录下各个文件与文件夹大小

sudo du -h --max-depth=1

解压文件到指定目录
unzip ./relpose++/relposepp_weights.zip -d weights

查看指定文件创建时间

ls -l --time-style=full-iso /home/jiantong/project/python/GRM/video_image_generated/cup/images/000092.png

查看某一目录中的文件数量

ls /home/mbzuai/anaconda3/envs/droidenv | wc -l

ls /home/jiantong/local/miniconda3/envs/droidenv | wc -l

查看剩余磁盘空间

df -h /

### 安装公钥

jiantong@mbzuai-metaverse-01:~/.ssh$ cat ./id_rsa.pub >> authorized_keys



cat C:\Users\zjt00\.ssh\id_rsa.pub | ssh jiantong@10.127.94.11 "mkdir -p ~/.ssh && touch ~/.ssh/authorized_keys && chmod -R go= ~/.ssh && cat >> ~/.ssh/authorized_keys"



```bash
cat C:\Users\zjt00\.ssh\id_rsa.pub | ssh jiantong@10.127.94.11 'cat >>.ssh/authorized_keys && echo \"Key copied\"'
```

### 解决连接github问题

https://zhuanlan.zhihu.com/p/521340971

~~~bash
$ vim ~/.ssh/config
```
# Add section below to it
Host github.com
  Hostname ssh.github.com
  Port 443
```
$ ssh -T git@github.com
Hi xxxxx! You've successfully authenticated, but GitHub does not
provide shell access.
~~~

[[SOLVED\] ssh could not resolve hostname github.com temporary failure in name resolution](https://codetryout.com/github-temporary-failure-in-name-resolution/)

\# cat /etc/resolv.conf 

nameserver 8.8.4.4 

nameserver 8.8.8.8

### nerfstudio

处理polycam数据
ns-process-data polycam --data ./cup_Polycam.zip --output-dir ./nerfstudio/data/nerfstudio/

处理realitycap数据

ns-process-data realitycapture --data ~/nerfstudio/data_raw/Cup --csv ~/nerfstudio/data_raw/cup_realitycapture/cup.csv --output-dir ~/nerfstudio/data/cup/ 

使用colmap处理原始图片数据

ns-process-data images --data ~/nerfstudio/data_raw/video5 --output-dir ~/nerfstudio/data/video5 --verbose --matching-method exhaustive --sfm-tool colmap --use-sfm-depth --include-depth-debug --same-dimensions | tee ~/nerfstudio/outputs/video5/log.txt

训练模型
ns-train nerfacto --data /home/jiantong/project/python/nerf_example_data/nerf_synthetic/lego

查看Nerfstudio结果

ns-viewer --load-config outputs/mouse_move/nerfacto/2023-09-01_180401/config.yml

渲染视频指令

ns-render camera-path --load-config outputs/Cup_KIRI/nerfacto/2023-08-29_184302/config.yml --camera-path-filename /home/jiantong/nerfstudio/data/nerfstudio/Cup_KIRI/camera_paths/2023-08-29_184302.json --output-path renders/Cup_KIRI/2023-08-29_184302.mp4

配置文件与检查点位置

Config File: ~/nerfstudio/data/nerfstudio/outputs/cup_polycam/nerfacto/2023-08-29_163856/config.yml
Checkpoint Directory: ~/nerfstudio/data/nerfstudio/outputs/cup_polycam/nerfacto/2023-08-29_163856/nerfstudio_models



### docker

删除docker

https://blog.csdn.net/CSDN_duomaomao/article/details/78587103

如果想看到所有的container，包括运行中的，以及未运行的或者说是沉睡镜像，则运行：

sudo docker ps -a

查看所有image

sudo docker images -a

start a container

sudo docker start ef24e891cc43

get into a container

sudo docker attach ef24e891cc43

docker exec -it ef24e891cc43 /bin/bash

push an image to dockerhub

sudo docker login
zjt000125@gmail.com
Wayni@000125

sudo docker tag nvcr.io/nvidian/bundlesdf:latest zjt0125/bundlesdf:v.1.0

sudo docker push zjt0125/bundlesdf:v.1.0

删除docker image

sudo docker image rm 9c7a54a9a43c



ffmpeg -i '/home/jiantong/project/python/XMem2/example_videos/switch/switch.MOV' '/home/jiantong/project/python/XMem2/example_videos/switch/switch.mp4'

### Jupyter notebook

为某一用户安装ipykernel
python -m ipykernel install --user --name seg



### Xmem

运行XMem分割视频
python eval.py --model './saves/XMem.pth' --generic_path ./Data --filenames 'video7' --dataset 'G' --output ./Outputs --size 480



### Colmap

```
sudo apt-get install \
    git \
    cmake \
    ninja-build \
    build-essential \
    libboost-program-options-dev \
    libboost-filesystem-dev \
    libboost-graph-dev \
    libboost-system-dev \
    libeigen3-dev \
    libflann-dev \
    libfreeimage-dev \
    libmetis-dev \
    libgoogle-glog-dev \
    libgtest-dev \
    libsqlite3-dev \
    libglew-dev \
    qtbase5-dev \
    libqt5opengl5-dev \
    libcgal-dev \
    libceres-dev | tee -a ~/log_install_colmap.txt
```



```
git clone https://github.com/colmap/colmap.git
```

export CUDACXX=/usr/local/cuda/bin/nvcc

cmake .. -GNinja -DCMAKE_CUDA_ARCHITECTURES=native

colmap 自动重建
colmap automatic_reconstructor --workspace_path ./ --image_path ./images | tee ./log.txt

特征提取

colmap feature_extractor --database_path ./database.db --image_path ./images | tee ./log.txt

特征匹配

colmap exhaustive_matcher --database_path ./database.db | tee -a ./log.txt

mkdir ./sparse

得到sparse结果

colmap mapper \
    --database_path ./database.db \
    --image_path ./images \
    --output_path ./sparse | tee -a ./log.txt

mkdir ./dense

# 
colmap image_undistorter \
    --image_path ./images \
    --input_path ./sparse/0 \
    --output_path ./dense \
    --output_type COLMAP \
    --max_image_size 2000 | tee -a ./log.txt

# 
colmap patch_match_stereo \
    --workspace_path ./dense \
    --workspace_format COLMAP \
    --PatchMatchStereo.geom_consistency true | tee -a ./log.txt

# 
colmap stereo_fusion \
    --workspace_path ./dense \
    --workspace_format COLMAP \
    --input_type geometric \
    --output_path ./dense/fused.ply | tee -a ./log.txt

# 
colmap poisson_mesher \
    --input_path ./dense/fused.ply \
    --output_path ./dense/meshed-poisson.ply | tee -a ./log.txt

#
colmap delaunay_mesher \
    --input_path ./dense \
    --output_path ./dense/meshed-delaunay.ply | tee -a ./log.txt



### 脚本

\# 特征提取

colmap feature_extractor --database_path ./database.db --image_path ./images --ImageReader.camera_model PINHOLE --ImageReader.single_camera 1 | tee ./log.txt

\# 特征匹配

colmap exhaustive_matcher --database_path ./database.db | tee -a ./log.txt

mkdir ./sparse

\# 得到sparse结果

colmap mapper \

  --database_path ./database.db \

  --image_path ./images \

  --output_path ./sparse | tee -a ./log.txt

22:12

zip -r plants_gaussian.zip plants_gaussian

### 分割nerf_synthetic数据集

python split.py --old_path=/home/jiantong/project/python/Nerf_Data_Process/nerf_synthetic/lego --new_path=/home/jiantong/project/python/Nerf_Data_Process/nerf_synthetic_split/lego_30 --split=val --split_ratio 0.3

### 修改环境变量
export PATH="/usr/local/cuda-11.8/bin/nvcc:$PATH"
export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"
echo 'export PATH="/usr/local/cuda-11.8/bin/nvcc:$PATH"' >> ~/.bashrc
source ~/.bashrc



### 3D Gaussian Splatting

安装3D-gaussian-splatting依赖库
conda env create --file environment.yml | tee -a ./log_install.txt

安装可视化工具

sudo apt install -y libglew-dev libassimp-dev libboost-all-dev libgtk-3-dev libopencv-dev libglfw3-dev libavdevice-dev libavcodec-dev libeigen3-dev libxxf86vm-dev libembree-dev | tee -a ./log_install.txt

cd SIBR_viewers

cmake -Bbuild . -DCMAKE_BUILD_TYPE=Release | tee -a ../log_install.txt

cmake --build build -j24 --target install | tee -a ../log_install.txt

sudo apt uninstall cmake

cmake 报错

export PATH=$PATH:/usr/local/cuda-11.8/bin/nvcc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.8/lib64

运行3D gaussian splatting

screen -r gaussian

conda activate gaussian_splatting

cd gaussian-splatting

训练

python train.py -s ./data/lego/lego_8_relpose_inverse -m ./output/lego/lego_8_relpose_inverse --save_iterations 10000 --checkpoint_iterations 10000 | tee -a ./data/lego/lego_8_relpose_inverse/log_train.txt

python train.py -s ./data/lego/lego_8 -m ./output/lego/lego_8 --save_iterations 10000 --checkpoint_iterations 10000 | tee -a ./data/lego/lego_8/log_train.txt

python train.py -s ./data/tissue_bag --save_iterations 10000 --checkpoint_iterations 10000

自带可视化工具

./SIBR_viewers/install/bin/SIBR_remoteGaussian_app -s ./output/tissue_bag/
./SIBR_viewers/install/bin/SIBR_remoteGaussian_app -s ./output/truck/
./SIBR_viewers/install/bin/SIBR_gaussianViewer_app -m ./outpyt/cup/

用nerfstudio执行可视化

cd ../nerfstudio

conda activate nerf_studio

python nerfstudio/scripts/gaussian_splatting/viewer.py --model-path ../gaussian-splatting/output/lego/lego_8
ssh -CNg -L 7007:127.0.0.1:7007 jiantong@10.127.94.15



### Ubuntu 录屏

ubuntu录屏取消时间限制
gsettings set org.gnome.settings-daemon.plugins.media-keys max-screencast-length 0

录屏快捷键

Shift+Ctrl+Alt+R
https://blog.csdn.net/weixin_51596276/article/details/121089138



### git相关

To pull a branch from GitHub to your local machine, you can follow these steps:

1. Open your terminal and navigate to the directory where you want to clone the repository.
2. Copy the URL of the repository from GitHub.
3. Type `git clone` followed by the URL of the repository and press Enter. This will create a local copy of the repository on your machine.
4. Navigate to the cloned repository using `cd`.
5. Type `git checkout` followed by the name of the branch you want to pull and press Enter. This will switch you to that branch.
6. Finally, type `git pull` and press Enter. This will pull the latest changes from that branch on GitHub to your local machine.



### 暂存区

3D点云重建0-00：MVSNet(R-MVSNet)-目录-史上最新无死角讲解：

https://blog.csdn.net/weixin_43013761/article/details/102852209

DTU数据集下载：

https://zhuanlan.zhihu.com/p/601757124



/home/jiantong/project/python/sparf/third_party/DenseMatching/model_selection.py

### 报错信息

python third_party/test_pdcnet_installation.py
third_party/../third_party/DenseMatching
Did not load moviepy
Applying PDC-Net to two images of DTU...
Model: PDCNet
Pre-trained-model: megadepth
GOCor: Local iter 3
GOCor: Global iter 3
/home/jiantong/local/bin/miniconda3/envs/sparf/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/jiantong/local/bin/miniconda3/envs/sparf/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Traceback (most recent call last):
  File "third_party/test_pdcnet_installation.py", line 127, in <module>
    test_model_on_image_pair(args, query_image, reference_image)
  File "third_party/test_pdcnet_installation.py", line 83, in test_model_on_image_pair
    confident_warped = overlay_semantic_mask(warped_query_image, ann=255 - confident_mask*255, color=color)
  File "/home/jiantong/project/python/sparf/third_party/../third_party/DenseMatching/utils_flow/visualization_utils.py", line 154, in overlay_semantic_mask
    im, ann = np.asarray(im, dtype=np.uint8), np.asarray(ann, dtype=np.int)
  File "/home/jiantong/local/bin/miniconda3/envs/sparf/lib/python3.8/site-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'int'.
`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations

### XMem++

cd ~/project/python/XMem2

分割视频

python interactive_demo.py --video example_videos/chair/chair.mp4

分割图片集

python interactive_demo.py --images example_videos/chair/JPEGImages

python interactive_demo.py --images example_videos/caps/JPEGImages --num-objects 2

默认结果保存位置

./workspace/chair

### AI Project

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export DATA_DIR='./test_datasets/'
```

```bash
CUDA_VISIBLE_DEVICES=0 python inference_local.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --test_data_dir=$DATA_DIR \
  --output_dir="./outputs/local_mapping"  \
  --suffix="object" \
  --template="a photo of a S" \
  --llambda="0.8" \·
  --global_mapper_path="./checkpoints/global_mapper.pt" \
  --local_mapper_path="./checkpoints/local_mapper.pt"
```

https://storage.googleapis.com/openimages/v6/test-annotations-bbox.csv

https://storage.googleapis.com/openimages/v6/test-annotations-object-segmentation.csv

scp -r jiantong@10.127.94.11:/home/jiantong/project/python/XMem2/workspace/cats C:\Users\zjt00\Downloads\cats



**How to fix the error on Ubuntu: ffmpeg: error while loading shared libraries: libopenh264.so.5: cannot open shared object file: No such file or directory?**

The error message you're encountering indicates that the FFmpeg program is unable to find the `libopenh264.so.5` shared library file, which is required for it to run properly. This issue can occur for several reasons, such as the library not being installed on your system, an incorrect library path, or a missing symbolic link. Here's how you can fix this error on Ubuntu:

### 1. Install the OpenH264 Library

First, try to install the OpenH264 library directly using your package manager. Open a terminal and run the following command:

```bash
sudo apt update
sudo apt install libopenh264-5
```

This command updates your package list and attempts to install the `libopenh264-5` package, which should provide the missing library.

### 2. Create a Symbolic Link

If the library is installed but FFmpeg can't find it, you might need to create a symbolic link to the library in a directory where FFmpeg looks for shared libraries. First, locate `libopenh264.so.5` using the `find` command:

```bash
sudo find / -name libopenh264.so.5
```

Assuming the library is found at `/usr/local/lib/libopenh264.so.5`, create a symbolic link to it in the `/usr/lib` directory (or another directory in your library path):

```bash
sudo ln -s /usr/local/lib/libopenh264-2.0.0-linux64.5.so /usr/lib/libopenh264.so.5
```

### 3. Install FFmpeg with Support for OpenH264

If you're still facing issues, it might be due to your FFmpeg build not supporting OpenH264 properly. You can try installing FFmpeg again, ensuring that it's compiled with OpenH264 support. You can use a PPA or compile FFmpeg from source with the appropriate flags for OpenH264.

### 4. Set `LD_LIBRARY_PATH`

If the library is installed in a non-standard location, you can tell the system where to find it by setting the `LD_LIBRARY_PATH` environment variable:

```bash
export LD_LIBRARY_PATH=/path/to/library:$LD_LIBRARY_PATH
```

Replace `/path/to/library` with the actual path to the directory containing `libopenh264.so.5`. You can add this line to your `.bashrc` or `.profile` to make the change permanent.

### 5. Check for Broken Dependencies

Sometimes, the issue might be due to broken dependencies. You can check and repair broken dependencies using the following command:

```bash
sudo apt --fix-broken install
```

This command will attempt to fix broken packages and might resolve issues with missing libraries.

If you've tried these steps and still encounter issues, you may need to provide more information about your FFmpeg installation and how it was installed (e.g., from the official repositories, a third-party PPA, or compiled from source). This can help in diagnosing the problem more accurately.

sudo wget -P /usr/local/lib http://ciscobinary.openh264.org/libopenh264-2.0.0-linux64.5.so.bz2

sudo ln -s /usr/local/lib/libopenh264-2.0.0-linux64.5.so.bz2 /usr/lib/libopenh264.so.5





### Colmap Command Arguments

colmap mapper -h
I0316 23:34:59.798074 3182038 option_manager.cc:849] COLMAP 3.9-dev (Commit 98940342 on 2023-12-25 with CUDA)
I0316 23:34:59.798099 3182038 option_manager.cc:851] Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`.
  -h [ --help ] 
  --random_seed arg (=0)
  --log_to_stderr arg (=1)
  --log_level arg (=0)
  --project_path arg
  --database_path arg
  --image_path arg
  --input_path arg
  --output_path arg
  --image_list_path arg
  --Mapper.min_num_matches arg (=15)
  --Mapper.ignore_watermarks arg (=0)
  --Mapper.multiple_models arg (=1)
  --Mapper.max_num_models arg (=50)
  --Mapper.max_model_overlap arg (=20)
  --Mapper.min_model_size arg (=10)
  --Mapper.init_image_id1 arg (=-1)
  --Mapper.init_image_id2 arg (=-1)
  --Mapper.init_num_trials arg (=200)
  --Mapper.extract_colors arg (=1)
  --Mapper.num_threads arg (=-1)
  --Mapper.min_focal_length_ratio arg (=0.10000000000000001)
  --Mapper.max_focal_length_ratio arg (=10)
  --Mapper.max_extra_param arg (=1)
  --Mapper.ba_refine_focal_length arg (=1)
  --Mapper.ba_refine_principal_point arg (=0)
  --Mapper.ba_refine_extra_params arg (=1)
  --Mapper.ba_min_num_residuals_for_multi_threading arg (=50000)
  --Mapper.ba_local_num_images arg (=6)
  --Mapper.ba_local_function_tolerance arg (=0)
  --Mapper.ba_local_max_num_iterations arg (=25)
  --Mapper.ba_global_images_ratio arg (=1.1000000000000001)
  --Mapper.ba_global_points_ratio arg (=1.1000000000000001)
  --Mapper.ba_global_images_freq arg (=500)
  --Mapper.ba_global_points_freq arg (=250000)
  --Mapper.ba_global_function_tolerance arg (=0)
  --Mapper.ba_global_max_num_iterations arg (=50)
  --Mapper.ba_global_max_refinements arg (=5)
  --Mapper.ba_global_max_refinement_change arg (=0.00050000000000000001)
  --Mapper.ba_local_max_refinements arg (=2)
  --Mapper.ba_local_max_refinement_change arg (=0.001)
  --Mapper.snapshot_path arg
  --Mapper.snapshot_images_freq arg (=0)
  --Mapper.fix_existing_images arg (=0)
  --Mapper.init_min_num_inliers arg (=100)
  --Mapper.init_max_error arg (=4)
  --Mapper.init_max_forward_motion arg (=0.94999999999999996)
  --Mapper.init_min_tri_angle arg (=16)
  --Mapper.init_max_reg_trials arg (=2)
  --Mapper.abs_pose_max_error arg (=12)
  --Mapper.abs_pose_min_num_inliers arg (=30)
  --Mapper.abs_pose_min_inlier_ratio arg (=0.25)
  --Mapper.filter_max_reproj_error arg (=4)
  --Mapper.filter_min_tri_angle arg (=1.5)
  --Mapper.max_reg_trials arg (=3)
  --Mapper.local_ba_min_tri_angle arg (=6)
  --Mapper.tri_max_transitivity arg (=1)
  --Mapper.tri_create_max_angle_error arg (=2)
  --Mapper.tri_continue_max_angle_error arg (=2)
  --Mapper.tri_merge_max_reproj_error arg (=4)
  --Mapper.tri_complete_max_reproj_error arg (=4)
  --Mapper.tri_complete_max_transitivity arg (=5)
  --Mapper.tri_re_max_angle_error arg (=5)
  --Mapper.tri_re_min_ratio arg (=0.20000000000000001)
  --Mapper.tri_re_max_trials arg (=1)
  --Mapper.tri_min_angle arg (=1.5)
  --Mapper.tri_ignore_two_view_tracks arg (=1)

### lab Server

ssh mbzuai@10.127.81.229

mbzuai 

ssh mbzuai@10.67.31.18

mbzuai

Use Ctrl + [ to remove the automatic item indentation set by Typora



Updates:

This is a summary of the work till now.

We can achieve reasonably good gaussian splatting reconstruction of the texture-less cup (rotated by hand in front of the camera) using the camera pose and point cloud obtained from droid-slam:



But If you examine the position of the cup's mouth, you will notice a significant misalignment:



You can also see the same misalignment in the initial point cloud:



That is, 3d gaussian fitted the pose error and point cloud error with its strong fitting capability.

It's good for accurate pose case, but very bad if camera pose is inaccurate.

I passed the render loss(3d gaussian's differentiable rendering) to the pose to optimize the pose during reconstruction, but got nearly unchanged reconstruction results.

I tried gaussian slam. Although it's good for scene reconstruction, its pose tracking and gaussian reconstruct failed in my cup video.

I also tried adding depth supervision to the gaussian reconstruction, but I can only get downsampled, very noised depth from droid-slam, so I don't think it'll work.

Now I feel that 3D Gaussian is not suitable for object pose estimation. The supervision provided by the object is limited compared to the scene, and the fitting capability of 3D Gaussian, along with its high degrees of freedom, make it worse. Even if there is an error in the camera pose, 3D Gaussian can still easily fit all input perspectives with the wrong modeling, without the need to optimize the camera pose. This is not conducive to the joint optimization of pose and object model.



This is my current idea:

I can design a three stage pipeline for reconstruction:

The first stage:

After using droid-slam to get coarse poses and point cloud, I can use point cloud process to refine the point cloud, and get a course SDF representation of the object by [shape-as-point](https://pengsongyou.github.io/sap).

The second stage:

For SDF can provide more accurate geometry constrain to pose and shape, we can jointly refine the SDF and poses.

The final stage:

Build 3d-gaussian using the refined pose, and extract mesh from SDF for downstream tasks.

I tried shape-as-point to refine the point cloud from droid-slam and build a SDF model for pose optimization.

表格：

- [x] 训练早期slot attention在CLEVR上的结果
- [x] 训练晚期slot attention在CLEVR上的结果
- [x] 训练晚期slot attention在COCO上的结果
- [x] 训练早期slot attention在COCO上的结果

图片：

- [x] 效果不错的CLEVR6结果
- [x] 条状MS COCO结果
- [x] 块状MS COCO结果
- [x] MAE重建结果
- [x] 原始Slot Attention结构（带输入输出样例）
- [x] teacher model训练pipeline
- [x] teacher model 的结构图



数据：

CLEVR：

Epoch: 195, Step: 80164:
       MSE       miou        mBO     FG-ARI
0  0.00156  74.695217  74.928525  95.629299

Epoch: 100, Step: 41309:
        MSE       miou        mBO     FG-ARI
0  0.002022  68.872088  69.054096  95.531633

COCO:

Epoch: 4, Step: 18480:
        MSE       miou        mBO   FG-ARI
0  0.343222  21.497475  21.791436  15.8462

Epoch: 15, Step: 59136:
        MSE       miou        mBO     FG-ARI
0  0.238552  17.436523  17.744293  11.873865



How to use index numpy.array with shape (200, 2), which represents 200 points' image coordinates, to index a numpy.ndarray depth image with shape (512, 512), get a result of shape (

```python
import numpy as np
from scipy.spatial.transform import Rotation

from kiui.op import safe_normalize
from kiui.typing import *

# convert between different world coordinate systems
def convert(
    pose, 
    target: Literal['unity', 'blender', 'opencv', 'colmap', 'opengl'] = 'unity', 
    original: Literal['unity', 'blender', 'opencv', 'colmap', 'opengl'] = 'opengl',
):
    """A method to convert between different world coordinate systems.

    Args:
        pose (np.ndarray): camera pose, float [4, 4].
        target (Literal[&#39;unity&#39;, &#39;blender&#39;, &#39;opencv&#39;, &#39;colmap&#39;, &#39;opengl&#39;], optional): from convention. Defaults to 'unity'.
        original (Literal[&#39;unity&#39;, &#39;blender&#39;, &#39;opencv&#39;, &#39;colmap&#39;, &#39;opengl&#39;], optional): to convention. Defaults to 'opengl'.

    Returns:
        np.ndarray: converted camera pose, float [4, 4].
    """
    
    if original == 'opengl':
        if target == 'unity':
            pose[2] *= -1
        elif target == 'blender':
            pose[2] *= -1
            pose[[1, 2]] = pose[[2, 1]]
        elif target in ['opencv', 'colmap']:
            pose[1:3] *= -1
    elif original == 'unity':
        if target == 'opengl':
            pose[2] *= -1
        elif target == 'blender':
            pose[[1, 2]] = pose[[2, 1]]
        elif target in ['opencv', 'colmap']:
            pose[1] *= -1
    elif original == 'blender':
        if target == 'opengl':
            pose[1] *= -1
            pose[[1, 2]] = pose[[2, 1]]
        elif target == 'unity':
            pose[[1, 2]] = pose[[2, 1]]
        elif target in ['opencv', 'colmap']:
            pose[2] *= -1
            pose[[1, 2]] = pose[[2, 1]]
    elif original in ['opencv', 'colmap']:
        if target == 'opengl':
            pose[1:3] *= -1
        elif target == 'unity':
            pose[1] *= -1
        elif target == 'blender':
            pose[1] *= -1
            pose[[1, 2]] = pose[[2, 1]]
    return pose


def look_at(campos, target, opengl=True):
    """construct pose rotation matrix by look-at.

    Args:
        campos (np.ndarray): camera position, float [3]
        target (np.ndarray): look at target, float [3]
        opengl (bool, optional): whether use opengl camera convention (forward direction is target --> camera). Defaults to True.

    Returns:
        np.ndarray: the camera pose rotation matrix, float [3, 3], normalized.
    """
   
    if not opengl:
        # forward is camera --> target
        forward_vector = safe_normalize(target - campos)
        up_vector = np.array([0, 1, 0], dtype=np.float32)
        right_vector = safe_normalize(np.cross(forward_vector, up_vector))
        up_vector = safe_normalize(np.cross(right_vector, forward_vector))
    else:
        # forward is target --> camera
        forward_vector = safe_normalize(campos - target)
        up_vector = np.array([0, 1, 0], dtype=np.float32)
        right_vector = safe_normalize(np.cross(up_vector, forward_vector))
        up_vector = safe_normalize(np.cross(forward_vector, right_vector))
    R = np.stack([right_vector, up_vector, forward_vector], axis=1)
    return R


def orbit_camera(elevation, azimuth, radius=1, is_degree=True, target=None, opengl=True):
    """construct a camera pose matrix orbiting a target with elevation & azimuth angle.

    Args:
        elevation (float): elevation in (-90, 90), from +y to -y is (-90, 90)
        azimuth (float): azimuth in (-180, 180), from +z to +x is (0, 90)
        radius (int, optional): camera radius. Defaults to 1.
        is_degree (bool, optional): if the angles are in degree. Defaults to True.
        target (np.ndarray, optional): look at target position. Defaults to None.
        opengl (bool, optional): whether to use OpenGL camera convention. Defaults to True.

    Returns:
        np.ndarray: the camera pose matrix, float [4, 4]
    """
    
    if is_degree:
        elevation = np.deg2rad(elevation)
        azimuth = np.deg2rad(azimuth)
    x = radius * np.cos(elevation) * np.sin(azimuth)
    y = - radius * np.sin(elevation)
    z = radius * np.cos(elevation) * np.cos(azimuth)
    if target is None:
        target = np.zeros([3], dtype=np.float32)
    campos = np.array([x, y, z]) + target  # [3]
    T = np.eye(4, dtype=np.float32)
    T[:3, :3] = look_at(campos, target, opengl)
    T[:3, 3] = campos
    return T


def undo_orbit_camera(T, is_degree=True):
    """ undo an orbital camera pose matrix to elevation & azimuth

    Args:
        T (np.ndarray): camera pose matrix, float [4, 4], must be an orbital camera targeting at (0, 0, 0)!
        is_degree (bool, optional): whether to return angles in degree. Defaults to True.

    Returns:
        Tuple[float]: elevation, azimuth, and radius.
    """
    
    campos = T[:3, 3]
    radius = np.linalg.norm(campos)
    elevation = np.arcsin(-campos[1] / radius)
    azimuth = np.arctan2(campos[0], campos[2])
    if is_degree:
        elevation = np.rad2deg(elevation)
        azimuth = np.rad2deg(azimuth)
    return elevation, azimuth, radius

# perspective matrix
def get_perspective(fovy, aspect=1, near=0.01, far=1000):
    """construct a perspective matrix from fovy.

    Args:
        fovy (float): field of view in degree along y-axis.
        aspect (int, optional): aspect ratio. Defaults to 1.
        near (float, optional): near clip plane. Defaults to 0.01.
        far (int, optional): far clip plane. Defaults to 1000.

    Returns:
        np.ndarray: perspective matrix, float [4, 4]
    """
    # fovy: field of view in degree.
    
    y = np.tan(np.deg2rad(fovy) / 2)
    return np.array(
        [
            [1 / (y * aspect), 0, 0, 0],
            [0, -1 / y, 0, 0],
            [
                0,
                0,
                -(far + near) / (far - near),
                -(2 * far * near) / (far - near),
            ],
            [0, 0, -1, 0],
        ],
        dtype=np.float32,
    )


def get_rays(pose, h, w, fovy, opengl=True, normalize_dir=True):
    """ construct rays origin and direction from a camera pose.

    Args:
        pose (np.ndarray): camera pose, float [4, 4]
        h (int): image height
        w (int): image width
        fovy (float): field of view in degree along y-axis.
        opengl (bool, optional): whether to use the OpenGL camera convention. Defaults to True.
        normalize_dir (bool, optional): whether to normalize the ray directions. Defaults to True.

    Returns:
        Tuple[np.ndarray]: rays_o and rays_d, both are float [h, w, 3]
    """
    # pose: [4, 4]
    # fov: in degree
    # opengl: camera front view convention

    x, y = np.meshgrid(np.arange(w), np.arange(h), indexing="xy")
    x = x.reshape(-1)
    y = y.reshape(-1)

    cx = w * 0.5
    cy = h * 0.5

    # objaverse rendering has fixed focal of 560 at resolution 512 --> fov = 49.1 degree
    focal = h * 0.5 / np.tan(0.5 * np.deg2rad(fovy))

    camera_dirs = np.stack([
        (x - cx + 0.5) / focal,
        (y - cy + 0.5) / focal * (-1.0 if opengl else 1.0),
        np.ones_like(x) * (-1.0 if opengl else 1.0),
    ], axis=-1) # [hw, 3]

    rays_d = camera_dirs @ pose[:3, :3].transpose(0, 1)  # [hw, 3]
    rays_o = np.expand_dims(pose[:3, 3], 0).repeat(rays_d.shape[0], 0)  # [hw, 3]

    if normalize_dir:
        rays_d = safe_normalize(rays_d)

    rays_o = rays_o.reshape(h, w, 3)
    rays_d = rays_d.reshape(h, w, 3)

    return rays_o, rays_d

class OrbitCamera:
    """ An orbital camera class.
    """
    def __init__(self, W, H, pos=np.zeros(3), rot=np.eye(3), fovy=60, near=0.01, far=200):
        """init function

        Args:
            W (int): image width
            H (int): image height
            r (int, optional): camera radius. Defaults to 2.
            fovy (int, optional): camera field of view in degree along y-axis. Defaults to 60.
            near (float, optional): near clip plane. Defaults to 0.01.
            far (int, optional): far clip plane. Defaults to 100.
        """
        self.W = W
        self.H = H
        self.radius = np.linalg.norm(pos)  # camera distance from center
        self.fovy = np.deg2rad(fovy)  # deg 2 rad
        self.near = near
        self.far = far
        self.center = np.array([0, 0, 0], dtype=np.float32)  # look at this point
        self.pos = pos
        self.rot = Rotation.from_matrix(rot)
        self.up = np.array([0, 1, 0], dtype=np.float32)  # need to be normalized!

    @property
    def fovx(self):
        """get the field of view in radians along x-axis

        Returns:
            float: field of view in radians along x-axis
        """
        return 2 * np.arctan(np.tan(self.fovy / 2) * self.W / self.H)

    @property
    def campos(self):
        """get the camera position

        Returns:
            np.ndarray: camera position, float [3]
        """
        return self.pose[:3, 3]


    @property
    def pose(self):
        """get the camera pose matrix (cam2world)

        Returns:
            np.ndarray: camera pose, float [4, 4]
        """
        # first move camera to radius
        res = np.eye(4, dtype=np.float32)
        # res[2, 3] = self.radius  # opengl convention...
        res[:3, 3] = self.pos
        # rotate
        # rot = np.eye(4, dtype=np.float32)
        # rot[:3, :3] = self.rot.as_matrix()
        # res = rot @ res
        res[:3, :3] = self.rot.as_matrix()
        # translate
        res[:3, 3] -= self.center
        return res

    
    @property
    def view(self):
        """get the camera view matrix (world2cam, inverse of cam2world)

        Returns:
            np.ndarray: camera view, float [4, 4]
        """
        return np.linalg.inv(self.pose)

    
    @property
    def perspective(self):
        """get the perspective matrix

        Returns:
            np.ndarray: camera perspective, float [4, 4]
        """
        y = np.tan(self.fovy / 2)
        aspect = self.W / self.H
        return np.array(
            [
                [1 / (y * aspect), 0, 0, 0],
                [0, -1 / y, 0, 0],
                [
                    0,
                    0,
                    -(self.far + self.near) / (self.far - self.near),
                    -(2 * self.far * self.near) / (self.far - self.near),
                ],
                [0, 0, -1, 0],
            ],
            dtype=np.float32,
        )

    # intrinsics
    @property
    def intrinsics(self):
        """get the camera intrinsics

        Returns:
            np.ndarray: intrinsics (fx, fy, cx, cy), float [4]
        """
        focal = self.H / (2 * np.tan(self.fovy / 2))
        return np.array([focal, focal, self.W // 2, self.H // 2], dtype=np.float32)

    
    @property
    def mvp(self):
        """get the MVP (model-view-perspective) matrix.

        Returns:
            np.ndarray: camera MVP, float [4, 4]
        """
        return self.perspective @ np.linalg.inv(self.pose)  # [4, 4]

    def orbit(self, dx, dy):
        """ rotate along camera up/side axis!

        Args:
            dx (float): delta step along x (up).
            dy (float): delta step along y (side).
        """
        side = self.rot.as_matrix()[:3, 0]
        # side = np.array([1, 0, 0], dtype=np.float32)  # need to be normalized!
        up = self.rot.as_matrix()[:3, 1]
        # rotvec_x = self.up * np.radians(-0.05 * dx)
        rotvec_x = up * np.radians(-0.05 * dx)
        rotvec_y = side * np.radians(-0.05 * dy)
        self.rot = Rotation.from_rotvec(rotvec_x) * Rotation.from_rotvec(rotvec_y) * self.rot

    def orbit_z(self, dz):
        z_axis = self.rot.as_matrix()[:3, 2]  # need to be normalized!
        rotvec_z = z_axis * np.radians(-0.05 * dz)
        self.rot = Rotation.from_rotvec(rotvec_z) * self.rot

    def scale(self, delta):
        """scale the camera.

        Args:
            delta (float): delta step.
        """
        self.radius *= 1.1 ** (-delta)

    def pan(self, dx, dy, dz=0):
        """pan the camera.

        Args:
            dx (float): delta step along x.
            dy (float): delta step along y.
            dz (float, optional): delta step along x. Defaults to 0.
        """
        # pan in camera coordinate system (careful on the sensitivity!)
        self.center += 0.0005 * self.rot.as_matrix()[:3, :3] @ np.array([dx, -dy, dz])

    def from_angle(self, elevation, azimuth, is_degree=True):
        """set the camera pose from elevation & azimuth angle.

        Args:
            elevation (float): elevation in (-90, 90), from +y to -y is (-90, 90)
            azimuth (float): azimuth in (-180, 180), from +z to +x is (0, 90)
            is_degree (bool, optional): whether the angles are in degree. Defaults to True.
        """
        if is_degree:
            elevation = np.deg2rad(elevation)
            azimuth = np.deg2rad(azimuth)
        x = self.radius * np.cos(elevation) * np.sin(azimuth)
        y = - self.radius * np.sin(elevation)
        z = self.radius * np.cos(elevation) * np.cos(azimuth)
        campos = np.array([x, y, z])  # [N, 3]
        rot_mat = look_at(campos, np.zeros([3], dtype=np.float32))
        self.rot = Rotation.from_matrix(rot_mat)
```


